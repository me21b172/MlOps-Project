{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f15024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataloader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332b5317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_port_in_use(8080)  # Example usage, check if port 8080 is in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d48c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = s.split(\"</a>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a06449a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Tech layoffs  Google Microsoft and more continue to fire employees'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.split(\"</a>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32269023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94171782 0.99603348]\n",
      " [0.09840676 0.16916325]\n",
      " [0.08900746 0.34375377]\n",
      " [0.57314964 0.8889423 ]\n",
      " [0.09393409 0.66382003]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = np.random.random((5,2))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48b461fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Column2",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "15a3d681-8ef9-4936-b60f-9d411c61cccd",
       "rows": [
        [
         "0",
         "0.9417178187944538",
         "0.9960334820618659"
        ],
        [
         "1",
         "0.09840675878512695",
         "0.16916324912352076"
        ],
        [
         "2",
         "0.08900745680227862",
         "0.3437537720348657"
        ],
        [
         "3",
         "0.5731496439446037",
         "0.8889422989948589"
        ],
        [
         "4",
         "0.09393409087493987",
         "0.6638200321613483"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.941718</td>\n",
       "      <td>0.996033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098407</td>\n",
       "      <td>0.169163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.343754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.573150</td>\n",
       "      <td>0.888942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093934</td>\n",
       "      <td>0.663820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column1   Column2\n",
       "0  0.941718  0.996033\n",
       "1  0.098407  0.169163\n",
       "2  0.089007  0.343754\n",
       "3  0.573150  0.888942\n",
       "4  0.093934  0.663820"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(n,columns=['Column1', 'Column2'])\n",
    "# df.reset_index('Column1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ee3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}\n",
    "reverse_map = {0:'business', 1:'entertainment', 2:'politics', 3:'sport', 4:'tech'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    \n",
    "    def _init_(self,num_filters,hidden_size,filter_size=3,num_classes=10,activation_function='ReLU',n_blocks=5,filter_organisation_factor=2,dropout=0.4,batch_norm=True):\n",
    "        \"\"\"\n",
    "        Custom CNN model for image classification.\n",
    "        Args:\n",
    "            num_filters (int): Number of filters in the first convolutional layer.\n",
    "            size_filter (int): Size of the convolutional filters.\n",
    "            num_classes (int): Number of output classes.\n",
    "            activation (str): Activation function to use .\n",
    "            n_blocks (int): Number of convolutional blocks.\n",
    "        \"\"\"\n",
    "        super(CustomCNN,self)._init_()\n",
    "        self.conv = nn.ModuleList()\n",
    "        self.activation = {\"ReLU\": nn.ReLU(), \"GeLU\": nn.GELU(), \"SiLU\": nn.SiLU(), \"Mish\": nn.Mish()}\n",
    "        in_channel = 3\n",
    "        # convolutional blocks\n",
    "        for i in range(n_blocks):\n",
    "            in_channel = num_filters*(filter_organisation_factor**(i-1)) if i>0 else 3\n",
    "            in_channel = int(in_channel)\n",
    "            numberOfFilters = num_filters*(filter_organisation_factor**(i))\n",
    "            numberOfFilters = int(numberOfFilters)\n",
    "            # start of the convolutional block\n",
    "            # convolution layer\n",
    "            self.conv.append(nn.Conv2d(in_channels = in_channel, out_channels=numberOfFilters, kernel_size=filter_size, stride=1))\n",
    "            # batch normalization layer\n",
    "            if batch_norm:\n",
    "                self.conv.append(nn.BatchNorm2d(numberOfFilters))\n",
    "            # activation layer\n",
    "            self.conv.append(self.activation[activation_function])\n",
    "            # max pooling layer\n",
    "            self.conv.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # flattening last layer\n",
    "        self.conv.append(nn.AdaptiveAvgPool2d(1))\n",
    "        self.conv.append(nn.Flatten())\n",
    "        # fully connected layer\n",
    "        in_channel = num_filters*(filter_organisation_factor**(n_blocks-1))\n",
    "        in_channel = int(in_channel)    \n",
    "        self.conv.append(nn.Linear(in_channel, hidden_size))\n",
    "        self.conv.append(self.activation['GeLU'])\n",
    "        # dropout layer\n",
    "        self.conv.append(nn.Dropout(dropout))\n",
    "        self.conv.append(nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.conv:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def train_model(self, \n",
    "                   train_loader: DataLoader, \n",
    "                   val_loader: DataLoader, \n",
    "                   epochs: int, \n",
    "                   learning_rate: float, \n",
    "                   device: torch.device,\n",
    "                   test_loader: DataLoader = None,\n",
    "                   criterion: nn.Module = nn.CrossEntropyLoss(),\n",
    "                   optimizer_class: optim.Optimizer = optim.Adam):\n",
    "        \"\"\"\n",
    "        Train the model with accuracy evaluation.\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader for training data.\n",
    "            val_loader (DataLoader): DataLoader for validation data.\n",
    "            epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimizer.\n",
    "            device (torch.device): Device to train on ('cuda' or 'cpu').\n",
    "            criterion (nn.Module): Loss function.\n",
    "            optimizer_class (optim.Optimizer): Optimizer class (e.g., Adam, SGD).\n",
    "        Returns:\n",
    "            Dict[str, List[float]]: Dictionary containing training/validation losses and accuracies.\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        optimizer = optimizer_class(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_acc':[]\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_correct += (outputs.argmax(1) == labels).sum().item()\n",
    " \n",
    "                del loss\n",
    "                del inputs\n",
    "            # Calculate training metrics\n",
    "            epoch_train_loss = running_loss / total_samples\n",
    "            epoch_train_acc = running_correct / total_samples\n",
    "            history['train_loss'].append(epoch_train_loss)\n",
    "            history['train_acc'].append(epoch_train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    batch_size = inputs.size(0)\n",
    "                    val_total += batch_size\n",
    "                    \n",
    "                    outputs = self(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * batch_size\n",
    "                    val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                    del loss\n",
    "                    del inputs\n",
    "            if test_loader is not None:\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in test_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        batch_size = inputs.size(0)\n",
    "                        test_total += batch_size\n",
    "                        \n",
    "                        outputs = self(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                        del loss\n",
    "                        del inputs\n",
    "            # Calculate validation metrics\n",
    "            epoch_val_loss = val_loss / val_total\n",
    "            epoch_val_acc = val_correct / val_total\n",
    "            epoch_test_acc = test_correct / test_total if test_loader is not None else None\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "            history['val_acc'].append(epoch_val_acc)\n",
    "            history['test_acc'].append(epoch_test_acc)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f'Epoch {epoch+1}/{epochs}: '\n",
    "                  f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | '\n",
    "                  f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    " \n",
    "        return history\n",
    "    \n",
    "    def predict(self, \n",
    "                test_loader: DataLoader, \n",
    "                device: torch.device) :\n",
    "        \"\"\"\n",
    "        Make predictions on test data.\n",
    "        Args:\n",
    "            test_loader (DataLoader): DataLoader for test data.\n",
    "            device (torch.device): Device to use for prediction.\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Predictions and true labels.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        self.to(device)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = self(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        \n",
    "        return torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "def transform_image(dataAugmentation=False):\n",
    "    if dataAugmentation:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        return transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def data_loader(data_dir, batch_size, dataAugmentation, num_workers=3):\n",
    "    # Load the full training dataset\n",
    "    full_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform_image(dataAugmentation=dataAugmentation))\n",
    "    targets = full_dataset.targets  # class labels for stratification\n",
    "\n",
    "    # Stratified split: 80% train, 20% val\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(full_dataset.samples, targets))\n",
    "\n",
    "    # Subsets\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "    # Override transform for val set (no augmentation)\n",
    "    val_dataset.dataset.transform = transform_image(dataAugmentation=False)\n",
    "\n",
    "    # Test dataset\n",
    "    test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform_image(dataAugmentation=False))\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed57585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (1): GELU(approximate='none')\n",
      "  (2): Linear(in_features=512, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, DistilBertConfig, DistilBertForSequenceClassification\n",
    "\n",
    "# Load model in half precision\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Custom config (5 output classes + FP16 if CUDA)\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=5,  # Your 5 classes\n",
    "    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32\n",
    ")\n",
    "\n",
    "# Load pretrained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "# ====== KEY MODIFICATION ======\n",
    "# 1. Add a new 256-unit layer before the classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(768, 512),  # New layer (768 -> 256)\n",
    "    nn.GELU(),            # Activation\n",
    "    nn.Linear(512, 5)     # Final classifier (256 -> 5 classes)\n",
    ").to(device)\n",
    "\n",
    "# 2. Freeze all layers EXCEPT the new classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze entire model\n",
    "\n",
    "# Unfreeze only the classifier\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify architecture\n",
    "print(model.classifier)\n",
    "# Should show:\n",
    "# Sequential(\n",
    "#   (0): Linear(in_features=768, out_features=256)\n",
    "#   (1): ReLU()\n",
    "#   (2): Linear(in_features=256, out_features=5)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d66d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (1): GELU(approximate='none')\n",
       "  (2): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b62aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a613ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.iloc[index]['Text'])  # Ensure string type\n",
    "        label = int(self.data.iloc[index]['Category'])  # Ensure integer\n",
    "        return {'text': text, 'label': label}\n",
    "\n",
    "def data_loader(data,batch_size=32):\n",
    "    \"\"\"\n",
    "    Load the data and create DataLoader objects for training and validation.\n",
    "    \"\"\"\n",
    "    # Load your dataset here\n",
    "    train = data.sample(frac=0.8, random_state=42)\n",
    "    val = data.drop(train.index)\n",
    "    train_dataset = NewDataset(train)\n",
    "    val_dataset = NewDataset(val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(model,data,lr,batch_size,num_epoch):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    train_loader, val_loader = data_loader(data=data,batch_size=batch_size)\n",
    "    total_cnt = 0\n",
    "    correct_cnt = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            labels = torch.tensor(batch['label'].tolist()).to(device)\n",
    "            outputs = model(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                labels = labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            correct_cnt += (torch.argmax(outputs.logits, dim=1) == labels).sum().item()\n",
    "            total_cnt += labels.size(0)\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch}, Loss: {loss.item()}, accuracy: {correct_cnt / total_cnt}\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            total_cnt = 0\n",
    "            correct_cnt = 0\n",
    "            for batch in val_loader:\n",
    "                inputs = tokenizer(batch['text'], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                labels = torch.tensor(batch['label'].tolist()).to(device)\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                correct_cnt += (torch.argmax(outputs.logits, dim=1) == labels).sum().item()\n",
    "                total_cnt += labels.size(0)\n",
    "            val_loss /= len(val_loader)\n",
    "            accuracy = correct_cnt / total_cnt\n",
    "            print(f\"Epoch {epoch+1}/{num_epoch}, Validation Loss: {val_loss}, Accuracy: {accuracy}\")\n",
    "    \n",
    "def predict(model,news:list):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(news, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    model.eval()\n",
    "    outputs = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask']\n",
    "    )\n",
    "    label = torch.argmax(outputs.logits, dim=1)\n",
    "    label = [reverse_map[label.data[i].item()] for i in range(len(label))]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'<a href=\"\"https://www.indiatoday.in/world/video/china-urges-us-cancel-reciprocal-tariffs-donald-trump-xi-jinping-2708501-2025-04-14\"\"> <img align=\"\"left\"\" alt=\"\"\"\" border=\"\"0\"\" height=\"\"180\"\" hspace=\"\"2\"\" src=\"\"https://akm-img-a-in.tosshub.com/indiatoday/images/video/202504/us-china-tariff-war-133601625-16x9_0.jpg?VersionId=2cszWtvdpM8FvhA9z5MTDbfWTnHDgvRv\"\" width=\"\"305\"\" /> </a> China urges US to cancel reciprocal tariffs calls for mutual respect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5dc941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82c7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6392111778259277, accuracy: 0.7139261744966443\n",
      "Epoch 1/5, Validation Loss: 0.5903841435909272, Accuracy: 0.7818791946308725\n",
      "Epoch 2/5, Loss: 0.6005663275718689, accuracy: 0.7644295302013423\n",
      "Epoch 2/5, Validation Loss: 0.5940935492515564, Accuracy: 0.7785234899328859\n",
      "Epoch 3/5, Loss: 0.883898138999939, accuracy: 0.7919463087248322\n",
      "Epoch 3/5, Validation Loss: 0.518662104010582, Accuracy: 0.8087248322147651\n",
      "Epoch 4/5, Loss: 0.1084352433681488, accuracy: 0.8\n",
      "Epoch 4/5, Validation Loss: 0.4229733467102051, Accuracy: 0.8590604026845637\n",
      "Epoch 5/5, Loss: 1.3128069639205933, accuracy: 0.8033557046979866\n",
      "Epoch 5/5, Validation Loss: 0.3608491882681847, Accuracy: 0.8624161073825504\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"BBC News Train.csv\")\n",
    "data = pd.concat([data[\"Text\"],data[\"Category\"]],axis=1)\n",
    "data['Category'] = data['Category'].map(label_map)\n",
    "train_model(model,data=data,lr=1e-2,batch_size=32,num_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = predict(model,[\"In a significant boost to India's futuristic military capability development, the Defence and Research Development Organisation (DRDO) Sunday showcased a 30-kilowatt laser-based weapon system designed to take down helicopters, swarm drones, and radars.\",\n",
    "                   \"It was Jasprit Bumrah's magic which saw Mumbai Indians come out on top, beating Delhi Capitals by 12 runs in their IPL 2025 fixture, at the Arun Jaitley Stadium in New Delhi on Sunday. Chasing 206 runs, DC had a sudden collapse in the 19th over as three run-outs in the 19th over saw Ashutosh Sharma (17), Kuldeep Yadav (1) and Mohit Sharma (0) lose their wickets in an error-prone end. In the run-chase, Delhi were dealt an early blow as Deepak Chahar removed opener Jake Fraser-McGurk for a golden duck in the first ball of the opening over. But since then, Abishek Porel and Karun Nair have stitched together a partnership, which has already gone past 100 runs, and the latter has also registered a 22-ball fifty\",\n",
    "                   \"Meet Laila Faisal, the rumoured girlfriend of star Indian batter Abhishek Sharma. A London-educated entrepreneur, she runs a luxury fashion brand and is making headlines for her glamorous style and growing connection with the IPL 2025 sensation.\",\n",
    "                   \"IPL 2025 star Abhishek Sharma has been turning heads not only with his cricketing prowess but also due to his rumored relationship with fashion entrepreneur Laila Faisal. Whispers about the duo intensified when Laila shared a heartfelt Instagram story celebrating Abhishek’s stunning 141-run knock against Punjab Kings — a performance hailed as one of the best this season. While the couple hasn't officially confirmed their relationship, their public chemistry is fueling ongoing buzz.\",\n",
    "                   \"As the trade war between the US and China escalates, Beijing has suspended the export of several critical rare earth elements, metals and magnets, threatening to choke off supplies to the West\",\n",
    "                   '<a href=\"\"https://www.indiatoday.in/trending-news/story/uk-woman-arrested-for-stealing-daughters-ipads-she-took-away-as-punishment-2708348-2025-04-13\"\"> <img align=\"\"left\"\" alt=\"\"\"\" border=\"\"0\"\" height=\"\"180\"\" hspace=\"\"2\"\" src=\"\"https://akm-img-a-in.tosshub.com/indiatoday/images/story/202504/uk-woman-arrested-for-stealing-her-daughters-ipads-133251723-16x9_0.jpg?VersionId=Rp60sqsQTuXSX24nsQhPYlilR71_ybeD\"\" width=\"\"305\"\" /> </a> UK woman arrested for stealing daughters iPads she took away as punishment',\n",
    "                   '<a href=\"\"https://www.indiatoday.in/world/video/china-urges-us-cancel-reciprocal-tariffs-donald-trump-xi-jinping-2708501-2025-04-14\"\"> <img align=\"\"left\"\" alt=\"\"\"\" border=\"\"0\"\" height=\"\"180\"\" hspace=\"\"2\"\" src=\"\"https://akm-img-a-in.tosshub.com/indiatoday/images/video/202504/us-china-tariff-war-133601625-16x9_0.jpg?VersionId=2cszWtvdpM8FvhA9z5MTDbfWTnHDgvRv\"\" width=\"\"305\"\" /> </a> China urges US to cancel reciprocal tariffs calls for mutual respect',\n",
    "                   \"Make Amit Shah apologise on Ambedkar, demands Chamala\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4878f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import pickle\n",
    "with open('model_pkl', 'wb') as files:\n",
    "    pickle.dump(model, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "233d4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_pkl' , 'rb') as f:\n",
    "    model1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc282a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = predict(model1,[\"In a significant boost to India's futuristic military capability development, the Defence and Research Development Organisation (DRDO) Sunday showcased a 30-kilowatt laser-based weapon system designed to take down helicopters, swarm drones, and radars.\",\n",
    "                   \"It was Jasprit Bumrah's magic which saw Mumbai Indians come out on top, beating Delhi Capitals by 12 runs in their IPL 2025 fixture, at the Arun Jaitley Stadium in New Delhi on Sunday. Chasing 206 runs, DC had a sudden collapse in the 19th over as three run-outs in the 19th over saw Ashutosh Sharma (17), Kuldeep Yadav (1) and Mohit Sharma (0) lose their wickets in an error-prone end. In the run-chase, Delhi were dealt an early blow as Deepak Chahar removed opener Jake Fraser-McGurk for a golden duck in the first ball of the opening over. But since then, Abishek Porel and Karun Nair have stitched together a partnership, which has already gone past 100 runs, and the latter has also registered a 22-ball fifty\",\n",
    "                   \"Meet Laila Faisal, the rumoured girlfriend of star Indian batter Abhishek Sharma. A London-educated entrepreneur, she runs a luxury fashion brand and is making headlines for her glamorous style and growing connection with the IPL 2025 sensation.\",\n",
    "                   \"IPL 2025 star Abhishek Sharma has been turning heads not only with his cricketing prowess but also due to his rumored relationship with fashion entrepreneur Laila Faisal. Whispers about the duo intensified when Laila shared a heartfelt Instagram story celebrating Abhishek’s stunning 141-run knock against Punjab Kings — a performance hailed as one of the best this season. While the couple hasn't officially confirmed their relationship, their public chemistry is fueling ongoing buzz.\",\n",
    "                   \"As the trade war between the US and China escalates, Beijing has suspended the export of several critical rare earth elements, metals and magnets, threatening to choke off supplies to the West\",\n",
    "                   '<a href=\"\"https://www.indiatoday.in/trending-news/story/uk-woman-arrested-for-stealing-daughters-ipads-she-took-away-as-punishment-2708348-2025-04-13\"\"> <img align=\"\"left\"\" alt=\"\"\"\" border=\"\"0\"\" height=\"\"180\"\" hspace=\"\"2\"\" src=\"\"https://akm-img-a-in.tosshub.com/indiatoday/images/story/202504/uk-woman-arrested-for-stealing-her-daughters-ipads-133251723-16x9_0.jpg?VersionId=Rp60sqsQTuXSX24nsQhPYlilR71_ybeD\"\" width=\"\"305\"\" /> </a> UK woman arrested for stealing daughters iPads she took away as punishment',\n",
    "                   '<a href=\"\"https://www.indiatoday.in/world/video/china-urges-us-cancel-reciprocal-tariffs-donald-trump-xi-jinping-2708501-2025-04-14\"\"> <img align=\"\"left\"\" alt=\"\"\"\" border=\"\"0\"\" height=\"\"180\"\" hspace=\"\"2\"\" src=\"\"https://akm-img-a-in.tosshub.com/indiatoday/images/video/202504/us-china-tariff-war-133601625-16x9_0.jpg?VersionId=2cszWtvdpM8FvhA9z5MTDbfWTnHDgvRv\"\" width=\"\"305\"\" /> </a> China urges US to cancel reciprocal tariffs calls for mutual respect',\n",
    "                   \"Make Amit Shah apologise on Ambedkar, demands Chamala\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7571848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(z)['text'].tolist()\n",
    "f = tokenizer(next(z)['text'], return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0, 2, 3, 0, 1, 1, 0, 0, 2, 2, 2, 4, 4, 1, 3, 3, 2, 2, 2, 1, 3, 4, 4,\n",
       "        0, 1, 3, 2, 3, 1, 0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(next(z)['label'].tolist()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e55619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for labels in next(z):\n",
    "    # print(txt)\n",
    "    print(labels)\n",
    "    # brea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc169d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Text samples: text\n",
      "Labels: label\n"
     ]
    }
   ],
   "source": [
    "# train_loader, val_loader = data_loader()\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, batch_data in enumerate(iter(train_loader)):\n",
    "    texts, labels = batch_data  # Unpack your batch\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Text samples:\", texts)  # First 2 texts in batch\n",
    "    print(\"Labels:\", labels)      # Corresponding labels\n",
    "    break  # Remove this to process all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87169f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BBC News Train.csv\")\n",
    "train = data.sample(frac=0.8, random_state=42)\n",
    "val = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb43661",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.concat([train[\"Text\"],train[\"Category\"]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z['Category'] = z['Category'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada5a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z['Category'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec56119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, ..., 0, 0, 2], shape=(1192,)),\n",
       " Index([1, 2, 4, 0, 3], dtype='int64'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.factorize(z['Category'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
